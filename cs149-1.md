### Prog1

本实验用多线程计算Mandelbrot Set，并与单线程进行性能比较。对于Mandelbrot Set，我们需要知道的是：

1. 每个像素的计算是独立进行的，和其他像素无关。
2. 不同像素的计算代价不一样。从图上看，越亮的点计算代价越大。

因此，问题的关键在于如何把计算任务分配给各线程。

最直接的想法是把整个区域横向平均分割成$n$块，每个线程按顺序取自己的一部分进行计算。从图1可以看出，中间部分的亮点数量更多，因此需要的计算代价也更大。因此线程号在中间的线程需要更长时间完成任务，成为性能瓶颈。

一种改进方法是让两侧的线程处理更多的行，中间的线程处理更少的行。但是如何取不同分割比例不是一个简单问题，而且不适用于图2。

因此，我让线程$i$处理$Row\% n == i $的那些行，这样做实现简单，需要修改一下`mandelbrotSerial`函数。

#### 测试结果

测试在i7-12700平台（8P4E）上进行，用`taskset`绑定到8个大核。

Naive实现：

| 线程数 | view1 | view2 |
| ------ | ----- | ----- |
| 1      | 1.00  | 1.00  |
| 2      | 1.95  | 1.66  |
| 3      | 1.62  | 2.13  |
| 4      | 2.39  | 2.51  |
| 5      | 2.42  | 2.84  |
| 6      | 3.17  | 3.23  |
| 7      | 3.30  | 3.61  |
| 8      | 3.90  | 4.00  |

Step实现：

| 线程数 | view1 | view2 |
| ------ | ----- | ----- |
| 1      | 0.99  | 0.99  |
| 2      | 1.95  | 1.94  |
| 3      | 2.86  | 2.84  |
| 4      | 3.81  | 3.78  |
| 5      | 4.66  | 4.63  |
| 6      | 5.58  | 5.51  |
| 7      | 6.35  | 6.31  |
| 8      | 7.27  | 7.20  |

继续增加线程数对性能提升没有帮助，因为只有8个核。

---

### Prog2

本实验旨在介绍SIMD指令的使用方式，需要使用框架提供的“模拟”SIMD指令完成一些功能。

#### clampedExpVector

该函数需要计算$\max (9.999999, value[i] ^ {exp[i]})$，由于每个分量的结果只和该分量的参数有关，不涉及不同分量之间的运算，因此只需要用SIMD指令模拟一下串行执行的逻辑即可。框架已经给出了一个`absVector`作为示例。

注意正确使用mask，为了处理`VECTOR_WIDTH`不能整除`N`的情况，应该用`_cs149_init_ones(min(VECTOR_WIDTH, *N* - i))`来设置全局的mask。示例代码`absVector`不能处理这种情况，也是因为这个原因。

#### arraySumVector

该函数需要计算$\sum\nolimits_{i=0}^NV_i$，这个函数的串行实现非常直接，但是使用SIMD进行计算的逻辑却完全不同，因为涉及到不同分量之间的运算。

做法是用`hdd`指令把相邻的分量加到一起，再用`interleave`指令把结果换到一起，再用`hdd`指令累加，重复这个过程直到得到最终结果，类似于一个归并过程。

这里假设了`VECTOR_WIDTH`是2的幂次，且`N`能被`VECTOR_WIDTH`整除。

#### 测试结果

测试$N=10000$时，改变SIMD指令操作的向量宽度，向量利用率的变化情况。向量利用率越低，说明运算过程中有越多的分量被mask屏蔽而没有参与计算。

| 向量宽度 | 向量利用率 |
| -------- | ---------- |
| 2        | 86.4%      |
| 4        | 81.8%      |
| 8        | 79.4%      |
| 16       | 78.2%      |
| 32       | 77.7%      |
| 64       | 77.5%      |
| 128      | 77.1%      |
| 256      | 76.5%      |

可以看出，向量宽度越高，利用率也越低，因为高维向量意味着处理一个进入了不同分支的分量会阻塞更多的分量。

---

### Prog3

本实验主要分析ispc程序的性能。ispc会使用SIMD指令加速执行（仍然是单核），ispc task还会启动多个task（类似于线程），跑在多个核上。

由于ispc使用宽度为8的AVX2指令，我们期望的理想性能提升是8倍，实际上只有2-4倍，这还是因为不同像素的计算开销不同导致的。和实验1不同的是，simd指令处理的向量对应到一组邻近的像素点，因此如果白点分布的比较离散，一个白点就会拖慢周围的其他黑点。因此图2比图1的提升幅度还要少一点。

在使用了ispc task后，我观测到task数目为16时性能提升达到峰值。这个行为有些奇怪，因为物理核数是8。

---

### Prog4

本实验以牛顿迭代法求平方根为例，分析SIMD的best/worst case。在baseline中，需要开方的输入数组是随机生成的。

对于best case，把所有输入置为2.99999，因为接近3的数迭代次数最多，计算代价最大，SIMD最能体现出针对串行程序的优势。此外，整个输入数组相同意味着控制流不会进入不同分支，屏蔽了SIMD的劣势。

对于worst case：

- 思路1：所有输入置为1，此时只需要1次迭代，瓶颈不在计算而在访存上，SIMD优势不明显。 
- 思路2：对于每组8个输入，设置其中1个为2.99999，其他都设为1。此时SIMD的执行速度被拖慢到对2.99999开方的速度，因此可能比串行还要慢。

#### 测试结果

| 测试     | ISPC提升比例 | 带task的ISPC提升比例 |
| -------- | ------------ | -------------------- |
| baseline | 3.77         | 30.49                |
| best     | 5.15         | 37.92                |
| worst1   | 1.38         | 1.96                 |
| worst2   | 0.89         | 6.67                 |

---

### Prog5

本实验分析SIMD对于一个简单计算任务的性能提升。在这个场景中，ispc几乎没有任何提升，而ispc task的提升也只有1.2倍左右，因为此时的瓶颈在内存读取上。

在计算访存次数时，框架认为每个元素对应4次访存操作。这是因为除了读取两个原数据以外，写回时会带来2次访存操作：

- write-back：cache未命中时，将需要修改的数据复制到cache，修改cache中的数据。写回时会带来另一次内存访问，共两次。
- write-through：cache未命中时，直接写入内存，同时将修改后的数据读进cache，也是两次。

---

### Prog4 Extra

本实验要求自己用SIMD指令实现一个浮点数开方运算。Intel为AVX指令集提供了一系列内置函数，编程者不需要手写汇编。

虽然实验要求用AVX2指令集进行实现，但似乎AVX2涉及的大部分是整型数运算，我用到的函数都来自AVX.

实现中，我模仿了标量运算的牛顿迭代法，用到的都是比较基本的AVX函数。

一些注意点：

1. AVX内置的算术运算函数不像proj2中的那些函数一样，有一个mask参数，因此分支操作比较麻烦，要依赖`_mm256_blendv_ps`（也可能是我没找到正确用法）。
2. 比较操作`_mm256_cmp_ps`的第三个参数，可以参考[这里](https://stackoverflow.com/questions/16988199/how-to-choose-avx-compare-predicate-variants)。我使用的是ordered, non-signaling的版本。
3. 我使用的都是宽度为256位的AVX函数，即同时处理8个单精度浮点数。

#### 测试结果

我的实现性能比不上ispc，AVX内置的开方函数`_mm256_sqrt_ps`性能远胜ispc.

| 测试       | 执行时间/ms | 提升比例 |
| ---------- | ----------- | -------- |
| serial     | 1658.965    | 1x       |
| ispc       | 428.850     | 3.87x    |
| ispc task  | 58.057      | 28.57x   |
| my AVX     | 686.149     | 2.42x    |
| AVX native | 19.576      | 84.75x   |
